## 신경망
딥러닝은 기본 층들을 쌓아서 구성한 신경망(Neural Network)이라는 모델을 사용하여 학습을 진행한다.<br>
신경망은 뉴런(Neuron)들로 이루어진 그룹을 의미함. 신경망은 원래 신경 생물학의 용어이며, 뉴런들의 끝이 다른 뉴런들과 연결된 구조이다.<br>
뇌 구조를 이해하는 것에서 영감을 받아, 딥러닝 모델의 핵심 개념을 설명하지만, 실제로 뇌를 모델링하여 만든 것은 아니고 그저 하나의 데이터 학습을 새로운 방식으로 하는 수학 모델이라고 생각하면 될

<br>

## Artificial Neural Network (ANN)
- 생물학적인 뉴런의 개념을 따서 만든 컴퓨팅(전산적)적인 모델.
- 두개의 층(Layers)이상으로 구성되어있다는 것이 특징.

<br>

## 뉴런 (Neuron)
- 생물학적인 뉴런의 개념에 기초한 수학적인 함수를 의미.
- 뉴런이 활성중인지에 따라서 활성함수가 결정됨.
- 해당 뉴런의 결과가 0이라면, 신호를 주고받지 않는 비활성화된 뉴런이라는 것을 알 수 있음 (activation function)

<br>

## 신경망의 구성

- Input Layer (입력층)
    - 입력 뉴런들로 구성된 층을 의미함.
    
- Output Layer (출력층)
    - 결과물을 생산하는 출력 뉴런을 의미함.
    
- Hidden Layer (은닉층)
    - 입력층과 출력층 사이에 있는 깊은 Layer을 의미함.
    
![신경망의 구성](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbsSSi0%2FbtqCMoQMU84%2FEdDxg79h62bGcjZXChVJZ1%2Fimg.png)

<br>

## 신경망 학습이란?

신경망 학습의 목표는 가중치(Weight)의 정확한 값을 찾는 것에 있다. <br>
어떤 층이 있을 때, 해당 층을 데이터가 거치면서 일어나는 변환은 해당 층의 가중치를 매개변수로 가지는 함수를 통하여 일어난다.

학습은 주어진 입력을 정확한 출력으로 매핑시키기 위해 모든 층의 가중치를 찾는 것을 의미힌다.

### 신경망의 학습 순서

1. 데이터를 입력받는다.
2. 층에 도달할 때마다 해당 층의 가중치를 사용하여 출력 값을 계산한다.
3. 모든 층에서 반복한다. 마지막으로는 예측 값이 나온다.
4. 예측 값은 실제 값과 비교하는 손실 함수를 통해서 점수를 계산한다.
5. 해당 점수를 피드백으로 삼아, 손실 점수가 감소되는 방향으로 가중치를 수정한다. (Optimizer)
6. 위의 과정을 반복(Loop)한다.
7. 손실 함수를 통해 계산되는 손실 점수를 최소화시키는 가중치를 찾아낸다.

<br>

## 가중치 (Weight)

- 뉴런 사이의 연결 강도를 의미함.
- 신경망이 훈련하는 동안, 업데이트 되어 weight가 변경됨.

![가중치](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbVewFr%2FbtqCIFl9ubi%2F9gQkECC1KRAQRc1vwMfAQ1%2Fimg.png)
  
<br>

## Bias
- Bias 뉴런의 가중치를 의미.
- 해당 값 또한, 훈련 시 업데이트됨.

<br>

## 활성화 함수 (Activation Function)
- 주어진 입력값들을 받은 뉴런의 출력값을 돌려주는 함수
- 현재 뉴런의 활성화에 따라 출력값을 결정함
    - 1은 뉴런을 활성화, 0은 뉴런을 비활성화
    
- 대표적인 활성화 함수
    - Sigmoid Function
    
        ![시그모이드 함수](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FboYHQ8%2FbtqBGfVYHd8%2FsRChnQZvs8a9jVtEHPM1Jk%2Fimg.png)
        - 초창기에 많이 사용된 활성화 함수로, S자형 곡선을 가지는 함수
        - AND, OR, XOR 연산을 다루는데 적합한 함수이다.
        - 또한 Binary Classification에 적합한 함수이다.
        - 출력값의 범위는 [0, 1]<br>
    <br>
    - ReLU (Rectified Linear Unit)
        ![ReLU 함수](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcuQ1uR%2FbtqBHyUOQAA%2FnUSjONqbdd3vlDk5mxmffk%2Fimg.png)
        - Sigmoid 대안으로 나온 활성화 함수로, 0이하의 수는 0을, 양수인 경우에는 양수를 그대로 반환하여 값의 왜곡이 적어짐.
        - 값의 범위는 [0, inf]